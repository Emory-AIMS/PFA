{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we choose the exponential function $e^{aq+b}+c$\n",
    "\n",
    "We opt for the exponential family based on two observations regarding the correlation between \n",
    "$\\varepsilon$ and $q$:\n",
    "1. the theoretical result (see Eq.(8)) involves a finite number of logarithmic and exponential functions;\n",
    "2. empirical findings (see Fig.5) suggest the existence of a convex relationship.\n",
    "\n",
    "In addition to $e^{aq+b}+c$, we also have assessed two variants and found that increased complexity in the expression leads to marginal improvement in $R^2$. Following Occam's razor principle, we adopt $e^{aq+b}+c$ as our fit function. \n",
    "\n",
    "Nevertheless, it is essential to explore superior alternatives for more **substantial** improvements in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39600 20400\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import datetime\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings # ignore warnings for clarity\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from myopacus import PrivacyEngine\n",
    "from myopacus.accountants.pdp_utils import GENERATE_EPSILONS_FUNC\n",
    "from strategies import FedAvg, PFA\n",
    "\n",
    "from datasets.fed_mnist import (\n",
    "    FedClass, \n",
    "    RawClass, \n",
    "    BaselineModel,\n",
    "    BaselineLoss,\n",
    "    Optimizer,\n",
    "    metric\n",
    ")\n",
    "\n",
    "NUM_LABELS = 10\n",
    "BATCH_SIZE = 512\n",
    "NUM_STEPS = 100\n",
    "LR = 0.01\n",
    "TARGET_DELTA = 0.0001\n",
    "MAX_GRAD_NORM = 1.0\n",
    "MAX_PHYSICAL_BATCH_SIZE = 512\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "project_abspath = os.path.abspath(os.path.join(os.getcwd(),\"..\"))\n",
    "data_path = os.path.join(project_abspath, \"datasets/fed_mnist/iid_10\")\n",
    "rawdata = RawClass(data_path=data_path)\n",
    "train_dataloader = FedClass(rawdata=rawdata, train=True, pooled=True)\n",
    "test_dataloader = FedClass(rawdata=rawdata, train=False, pooled=True)\n",
    "print(len(train_dataloader), len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(BaselineModel).to(device)\n",
    "criterion = BaselineLoss()\n",
    "optimizer = Optimizer(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "make_private_with_epsilon() missing 1 required keyword-only argument: 'target_epsilon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_444103/4237319309.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtarget_epsilons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_epsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtarget_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_DELTA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_GRAD_NORM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_physical_batch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: make_private_with_epsilon() missing 1 required keyword-only argument: 'target_epsilon'"
     ]
    }
   ],
   "source": [
    "target_epsilon = 1.0\n",
    "privacy_engine = PrivacyEngine(accountant=\"fed_rdp\", n_clients=1)\n",
    "model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_dataloader,\n",
    "    epochs=NUM_STEPS,\n",
    "    target_epsilon=target_epsilon,\n",
    "    target_delta=TARGET_DELTA,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    ")\n",
    "if self.max_physical_batch_size > 0:\n",
    "    with BatchMemoryManager(data_loader=train_dataloader,\n",
    "            max_physical_batch_size=self.max_physical_batch_size,\n",
    "            optimizer=optimizer) as memory_safe_dl:\n",
    "        train_dataloader = memory_safe_dl\n",
    "print(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standardize(M):\n",
    "    '''Compute the mean of every dimension of the whole dataset'''\n",
    "    [n, m] = M.shape\n",
    "    if m == 1:\n",
    "        print(m==1)\n",
    "        return M, np.zeros(n)\n",
    "    mean = np.dot(M,np.ones((m,1), dtype=np.float32)) / m\n",
    "    return M - mean, mean.flatten()\n",
    "\n",
    "def _eigen_by_lanczos(mat):\n",
    "    T, V = Lanczos(mat, self.lanczos_iter)\n",
    "    T_evals, T_evecs = np.linalg.eig(T)\n",
    "    idx = T_evals.argsort()[-1 : -(self.proj_dims+1) : -1]\n",
    "    Vk = np.dot(V.T, T_evecs[:,idx])\n",
    "    return Vk\n",
    "\n",
    "def svc(updates):\n",
    "    num_weights = len(updates)\n",
    "    shape_weights = [var.shape for var in updates]\n",
    "    print(num_weights, shape_weights)\n",
    "\n",
    "    for i in range(num_weights):\n",
    "        _updates_normlized, mean = _standardize(updates[i].T)\n",
    "        Vk = _eigen_by_lanczos(updates[i].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader_iter = iter(train_dataloader)\n",
    "\n",
    "_previous_state = None\n",
    "test_accs, i = [], 0\n",
    "while i < NUM_STEPS:\n",
    "    model.train()\n",
    "    try:\n",
    "        data, target = next(train_loader_iter)\n",
    "    except StopIteration:\n",
    "        train_loader_iter = iter(train_dataloader)\n",
    "        data, target = next(train_loader_iter)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(input=output, target=target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if len(privacy_engine.accountant) and (i == privacy_engine.accountant.history[-1][-1] - 1):\n",
    "        if i % 10 == 0:\n",
    "            if _previous_state is not None:\n",
    "                _next_state = _model._get_current_params()\n",
    "                updates = [\n",
    "                    new - old for new, old in zip(_next_state, _previous_state)\n",
    "                ]\n",
    "            _previous_state = model._get_current_params()\n",
    "            \n",
    "        i += 1\n",
    "        total_correct, total_points = 0, 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for data, target in iter(test_dataloader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "\n",
    "                test_correct = metric(y_true=target.detach().cpu().numpy(), y_pred=output.detach().cpu().numpy())\n",
    "                total_correct += test_correct\n",
    "                total_points += len(target)\n",
    "\n",
    "            test_accs.append(round(total_correct/total_points, 4))\n",
    "            print(f\"Step {i}: test_correct = {total_correct}/{total_points}, \" \\\n",
    "                  f\"test_acc = {round(total_correct/total_points, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cd2817a37aac10ac04dd4bd211d68db65c45a714a600fc3dbad943ddf1ad11a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
